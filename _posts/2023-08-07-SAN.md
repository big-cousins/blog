---
layout: post
title: Linux SAN基本概要
categories: Blog
keywords: linux SAN 
---

### 概要

SAN：Storage Area Network

- IO栈，IO栈图如下

![My helpful screenshot]({{"/images/posts/blog/san/san1.png" | absolute_url}}) 

- 块层

块层实现了应用程序和文件系统访问存储设备的接口。Linux块层向上为文件系统和块设备提供接口，使得上层能够以统一的方式访问各种类型的后端存储设备。同时，它也向下为设备驱动提供接口，让驱动层能够以一致的方式接受请求。块层的源码可以简单认为是"block"下子目录下的所有源码，这些源码可以分为两层，可以简单的称作bio层和request层。

- 块层之上

在深挖bio层之前，看看块层之上，这里“之上”意思是靠近用户空间（the top）,远离硬件(the bottom)，包括所有使用块层服务的代码。

![My helpful screenshot]({{"/images/posts/blog/san/san2.jpg" | absolute_url}}) 

通常，我们可以通过/dev目录下的块设备文件来访问块设备，在内核中块设备文件会映射到有一个S_IFBLK标记的inode。inode结构体的i_bdev域会指向一个代表目标设备的struct block_device对象。如下：

```c
struct inode {
    union {
        ......
        struct block_device* i_bdev;
        ......
    };
};
```

bio层

bio结构体如下

```c
/*
 * main unit of I/O for the block layer and lower layers (ie drivers and stacking drivers)
 */
struct bio {
   struct bio      *bi_next;       /* request queue link，用于链接处于同一request的bio */
   struct block_device* bi_bdev;   /* 该bio所请求的块设备 */
   unsigned long bi_rw;            /* 标志位，用于区分读写 */
   unsigned short bi_vcnt;         /* how many bio_vec's */
   unsigned short bi_idx;          /* current index into bvl_vec */
   unsigned int bi_phys_segments;  /* 合并后的segments数目 */
   unsigned int bi_size;           /* io大小，本io数据量，以字节为单位 */
   unsigned int bi_seg_front_size; /* 第一个可合并的segment的大小，与request合并相关 */
   unsigned int bi_seg_back_size;  /* 最后一个可合并segment的大小，与request合并相关*/
   bio_end_io_t *bi_end_io;        /* 该bio结束时的回调函数，一般用于通知调用者该bio的完成情况*/
   struct gendisk      *bi_disk;
   unsigned int        bi_opf;     /* bottom bits req flags,
                        * top bits REQ_OP. Use
                        * accessors.
                        */
   unsigned short      bi_flags;   /* status, etc and bvec pool number */
   unsigned short      bi_ioprio;
   unsigned short      bi_write_hint;
   blk_status_t        bi_status;
   u8          bi_partno;
   struct bvec_iter    bi_iter;
   atomic_t        __bi_remaining;
   void            *bi_private;
   #ifdef CONFIG_BLK_CGROUP
   /*
    * Represents the association of the css and request_queue for the bio.
    * If a bio goes direct to device, it will not have a blkg as it will
    * not have a request_queue associated with it.  The reference is put
    * on release of the bio.
      */
      struct blkcg_gq     *bi_blkg;
      struct bio_issue    bi_issue;
      #endif
      union {
      #if defined(CONFIG_BLK_DEV_INTEGRITY)
      struct bio_integrity_payload *bi_integrity; /* data integrity */
      #endif
      };
      unsigned short      bi_vcnt;    /* how many bio_vec's */
      /*
    * Everything starting with bi_max_vecs will be preserved by bio_reset()
      */
      unsigned short      bi_max_vecs;    /* max bvl_vecs we can hold */
      atomic_t        __bi_cnt;   /* pin count */
      struct bio_vec      *bi_io_vec; /* the actual vec list 向量数组指针 */
      struct bio_set      *bi_pool;
      /*
    * We can inline a number of vecs at the end of the bio, to avoid
    * double allocations for a small number of bio_vecs. This member
    * MUST obviously be kept at the very end of the bio.
      */
      struct bio_vec      bi_inline_vecs[0];
 };
```


一个BIO所请求的数据在块设备中是连续的，对于不连续的数据块需要放到多个BIO中

一个BIO所携带的数据大小是有上限的，该上限值由bi_max_vecs间接指定，超过上限的数据块必须放到多个BIO中。

BIO、bi_io_vec、page之间的关系

![My helpful screenshot]({{"/images/posts/blog/san/san3.png" | absolute_url}}) 

<u>*关于bi_io_vec和bi_inline_vecs的关系*</u>

一个bio会有两个字段用来描述bio携带的数据，对于没有深入了解的人来讲会比较困惑，其实也比较容易理解，bio结构再申请内存的时候会多申请4个bvec的位置跟随bio结构体上，这就是bi_inline_vecs，它是用来存放内联数据（不能太多，因为申请了不用就会造成内存浪费），当往该bio注入数据时，小块的bio会直接使用这个内联的数据区域保存小于4个bvec的信息，而无需重新调用kmalloc申请内存，减少了一次内存申请操作，牺牲一小部分内存达到对小bio的加速（内存申请过程很长且相对较慢），然后为了兼容后端的其他接口，会直接将bi_io_vec指针直接指向bi_inline_vecs所在的位置，目的就是告诉后端，数据存放在bi_inline_vecs位置上。如果超过4个以上的bvec才足够完整描述本次IO的全部数据，那么bi_inline_vecs字段是被直接忽略的，尽管它内存很早就申请好了， 但是并不会被采用，而是需要重新自内存管理单元重新申请足额的内存保存这一次的IO数据，这就是前面说的一点点的内存浪费的原因。

Linux中块设备用struct gendisk表示，即一个通用磁盘generic disk。这个结构体也没包含太多信息，主要起承上启下的作用，上承文件系统，下启块层。往上层走，一个gendisk对象会关联到block_device对象，如我们上文所述，block_device对象被链接到/dev目录下的inode中。如果一个物理块设备包含多个分区，也就是有多个分区表，那么这个gendisk对象就对应多个block_device对象。其中，有一个block_device对象代表着整个物理磁盘gendisk，而其他block_device各代表gendisk中的一个分区。

struct bio是bio层的一个重要数据结构，用来表示来自bloc_device对象的读写请求，以及各种其他的控制请求，然后把这些请求传达到驱动层。一个bio对象包括的信息有目标设备，设备地址空间上的偏移量，请求类型（通常是读或写），读写大小，和用来存放数据的内存区域。在linux4.14之前，bio对象是用block_device来表示目标设备的。而现在bio对象包含一个指向gendisk结构体的指针和分区号，这些可通过做更突出了gendisk结构体的核心地位，更自然一些。bio一旦构造好，上层代码就可以调用generic_make_request()或submit_bio()提交给bio层处理。通常上层代码不会等待请求处理完成，而是把请求放到块设备队列上就返回了。generic_make_request有时可能阻塞一小会，比如在等待内存分配的时候，这样想可能更容易理解，它也许要等待一些已经在队列上的请求处理完成，然后腾出空间。

### 调试

#### 日志开关

**<u>block层开关</u>**

启动block_dump通用块层debug日志开关，实时监控IO

```shell
echo 1 > /proc/sys/vm/block_dump
```

通过dmesg信息可以看到IO正在读/写哪些文件或者块设备，有进程号，IO大小，起始扇区等。

**<u>SCSI层日志开关</u>**

```shell
scsi_logging_level -s -E 7					# 开启错误日志
scsi_logging_level -s -T 7					# 开启超时日志
scsi_logging_level -s -a 7					# 打开所有日志
scsi_logging_level -s -a 0					# 关闭所有日志
```

**<u>iscsi客户端</u>**

```shell
# 开启日志
echo 1 > /sys/module/libiscsi/parameters/debug_libiscsi_session
echo 1 > /sys/module/libiscsi/parameters/debug_libiscsi_eh
echo 1 > /sys/module/libiscsi/parameters/debug_libiscsi_conn
echo 1 > /sys/module/libiscsi_tcp/parameters/debug_libiscsi_tcp

# 关闭日志
echo 0 > /sys/module/libiscsi/parameters/debug_libiscsi_session
echo 0 > /sys/module/libiscsi/parameters/debug_libiscsi_eh
echo 0 > /sys/module/libiscsi/parameters/debug_libiscsi_conn
echo 0 > /sys/module/libiscsi_tcp/parameters/debug_libiscsi_tcp
```

